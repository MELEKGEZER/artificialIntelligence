{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:54:37.292144Z",
     "start_time": "2025-05-09T11:54:20.972777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import chardet\n",
    "import grpc\n",
    "import zemberek_grpc.morphology_pb2 as z_morphology\n",
    "import zemberek_grpc.morphology_pb2_grpc as z_morphology_g\n",
    "from functools import lru_cache"
   ],
   "id": "45e94ff5e922e933",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melek\\PycharmProjects\\MyDataset\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:54:43.606669Z",
     "start_time": "2025-05-09T11:54:37.303666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# TÃ¼rkÃ§e stopwords'ler\n",
    "turkish_stopwords = set(stopwords.words('turkish'))\n",
    "\n",
    "# GeniÅŸletilmiÅŸ TÃ¼rkÃ§e stopwords listesi (ilk kodla aynÄ±)\n",
    "extended_turkish_stopwords = {\n",
    "    'a', 'acaba', 'acep', 'adamakÄ±llÄ±', 'adeta', 'ait', 'altmÄ±ÅŸ', 'altÄ±',\n",
    "    'ama', 'amma', 'anca', 'ancak', 'arada', 'artÄ±k', 'aslÄ±nda', 'aynen', 'ayrÄ±ca',\n",
    "    'az', 'aÃ§Ä±kÃ§a', 'aÃ§Ä±kÃ§asÄ±', 'bana', 'bari', 'bazen', 'bazÄ±', 'baÅŸkasÄ±',\n",
    "    'belki', 'ben', 'benden', 'beni', 'benim', 'beri', 'beriki', 'beÅŸ',\n",
    "    'bilcÃ¼mle', 'bile', 'bin', 'binaen', 'binaenaleyh', 'bir', 'biraz',\n",
    "    'birazdan', 'birbiri', 'birden', 'birdenbire', 'biri', 'birice', 'birileri',\n",
    "    'birisi', 'birkaÃ§', 'birkaÃ§Ä±', 'birkez', 'birlikte', 'birÃ§ok', 'birÃ§oÄŸu',\n",
    "    'bir ÅŸey', 'bir ÅŸeyi', 'birÅŸey', 'birÅŸeyi', 'bitevi', 'biteviye',\n",
    "    'bittabi', 'biz', 'bizatihi', 'bizce', 'bizcileyin', 'bizden', 'bize', 'bizi',\n",
    "    'bizim', 'bizimki', 'bizzat', 'boÅŸuna', 'bu', 'buna', 'bunda', 'bundan',\n",
    "    'bunlar', 'bunlarÄ±', 'bunlarÄ±n', 'bunu', 'bunun', 'buracÄ±kta', 'burada',\n",
    "    'buradan', 'burasÄ±', 'bÃ¶yle', 'bÃ¶ylece', 'bÃ¶ylecene', 'bÃ¶ylelikle',\n",
    "    'bÃ¶ylemesine', 'bÃ¶ylesine', 'bÃ¼sbÃ¼tÃ¼n', 'bÃ¼tÃ¼n', 'cuk', 'cÃ¼mlesi', 'da',\n",
    "    'daha', 'dahi', 'dahil', 'dahilen', 'daima', 'dair', 'dayanarak', 'de', 'defa',\n",
    "    'dek', 'demin', 'demincek', 'deminden', 'denli', 'derakap', 'derhal', 'derken',\n",
    "    'deÄŸil', 'deÄŸin', 'diye', 'diÄŸer', 'diÄŸeri', 'doksan', 'dokuz',\n",
    "    'dolayÄ±', 'dolayÄ±sÄ±yla', 'doÄŸru', 'dÃ¶rt', 'edecek', 'eden', 'ederek', 'edilecek',\n",
    "    'ediliyor', 'edilmesi', 'ediyor', 'elbet', 'elbette', 'elli', 'emme', 'en',\n",
    "    'enikonu', 'epey', 'epeyce', 'epeyi', 'esasen', 'esnasÄ±nda', 'etmesi', 'etraflÄ±',\n",
    "    'etraflÃ­ca', 'etti', 'ettiÄŸi', 'ettiÄŸini', 'evleviyetle', 'evvel', 'evvela',\n",
    "    'evvelce', 'evvelden', 'evvelemirde', 'evveli', 'eÄ‘er', 'eÄŸer', 'fakat',\n",
    "    'filanca', 'gah', 'gayet', 'gayetle', 'gayri', 'gayrÄ±', 'gelgelelim', 'gene',\n",
    "    'gerek', 'gerÃ§i', 'geÃ§ende', 'geÃ§enlerde', 'gibi', 'gibilerden', 'gibisinden',\n",
    "    'gine', 'gÃ¶re', 'gÄ±rla', 'hakeza', 'halbuki', 'halen', 'halihazÄ±rda', 'haliyle',\n",
    "    'handiyse', 'hangi', 'hangisi', 'hani', 'hariÃ§', 'hasebiyle', 'hasÄ±lÄ±', 'hatta',\n",
    "    'hele', 'hem', 'henÃ¼z', 'hep', 'hepsi', 'her', 'herhangi', 'herkes', 'herkesin',\n",
    "    'hiÃ§', 'hiÃ§bir', 'hiÃ§biri', 'hoÅŸ', 'hulasaten', 'iken', 'iki', 'ila', 'ile',\n",
    "    'ilen', 'ilgili', 'ilk', 'illa', 'illaki', 'imdi', 'indinde', 'inen', 'insermi',\n",
    "    'ise', 'ister', 'itibaren', 'itibariyle', 'itibarÄ±yla', 'iyi', 'iyice', 'iyicene',\n",
    "    'iÃ§in', 'iÅŸ', 'iÅŸte', 'kadar', 'kaffesi', 'kah', 'kala', 'kannÄ±mca',\n",
    "    'karÅŸÄ±n', 'katrilyon', 'kaynak', 'kaÃ§Ä±', 'kelli', 'kendi', 'kendilerine',\n",
    "    'kendini', 'kendisi', 'kendisine', 'kendisini', 'kere', 'kez', 'keza',\n",
    "    'kezalik', 'keÅŸke', 'keÅ£ke', 'ki', 'kim', 'kimden', 'kime', 'kimi', 'kimisi',\n",
    "    'kimse', 'kimsecik', 'kimsecikler', 'kÃ¼lliyen', 'kÄ±rk',\n",
    "    'kÄ±saca', 'lakin', 'leh', 'lÃ¼tfen', 'maada', 'madem', 'mademki', 'mamafih',\n",
    "    'mebni', 'meÄŸer', 'meÄŸerki', 'meÄŸerse', 'milyar', 'milyon', 'mu',\n",
    "    'mÃ¼', 'mi', 'mÄ±', 'nasÄ±l', 'nasÄ±lsa', 'nazaran', 'naÅŸi', 'ne', 'neden',\n",
    "    'nedeniyle', 'nedenle', 'nedense', 'nerde', 'nerden', 'nerdeyse', 'nere',\n",
    "    'nerede', 'nereden', 'neredeyse', 'neresi', 'nereye', 'netekim', 'neye', 'neyi',\n",
    "    'neyse', 'nice', 'nihayet', 'nihayetinde', 'nitekim', 'niye', 'niÃ§in', 'o',\n",
    "    'olan', 'olarak', 'oldu', 'olduklarÄ±nÄ±', 'oldukÃ§a', 'olduÄŸu', 'olduÄŸunu',\n",
    "    'olmadÄ±', 'olmadÄ±ÄŸÄ±', 'olmak', 'olmasÄ±', 'olmayan', 'olmaz', 'olsa', 'olsun',\n",
    "    'olup', 'olur', 'olursa', 'oluyor', 'on', 'ona', 'onca', 'onculayÄ±n', 'onda',\n",
    "    'ondan', 'onlar', 'onlardan', 'onlarÄ±', 'onlarÄ±n', 'onu',\n",
    "    'onun', 'oracÄ±k', 'oracÄ±kta', 'orada', 'oradan', 'oranca', 'oranla', 'oraya',\n",
    "    'otuz', 'oysa', 'oysaki', 'pek', 'pekala', 'peki', 'pekÃ§e', 'peyderpey', 'raÄŸmen',\n",
    "    'sadece', 'sahi', 'sahiden', 'sana', 'sanki', 'sekiz', 'seksen', 'sen', 'senden',\n",
    "    'seni', 'senin', 'siz', 'sizden', 'sizi', 'sizin', 'sonra', 'sonradan',\n",
    "    'sonralarÄ±', 'sonunda', 'tabii', 'tam', 'tamam', 'tamamen', 'tamamÄ±yla',\n",
    "    'tarafÄ±ndan', 'tek', 'trilyon', 'tÃ¼m', 'var', 'vardÄ±', 'vasÄ±tasÄ±yla', 've',\n",
    "    'velev', 'velhasÄ±l', 'velhasÄ±lÄ±kelam', 'veya', 'veyahut', 'ya', 'yahut',\n",
    "    'yakinen', 'yakÄ±nda', 'yakÄ±ndan', 'yakÄ±nlarda', 'yalnÄ±z', 'yalnÄ±zca', 'yani',\n",
    "    'yapacak', 'yapmak', 'yaptÄ±', 'yaptÄ±klarÄ±', 'yaptÄ±ÄŸÄ±', 'yaptÄ±ÄŸÄ±nÄ±', 'yapÄ±lan',\n",
    "    'yapÄ±lmasÄ±', 'yapÄ±yor', 'yedi', 'yeniden', 'yenilerde', 'yerine',\n",
    "    'yetmiÅŸ', 'yine', 'yirmi', 'yok', 'yoksa', 'yoluyla', 'yÃ¼z', 'yÃ¼zÃ¼nden',\n",
    "    'zarfÄ±nda', 'zaten', 'zati', 'zira', 'Ã§abuk', 'Ã§abukÃ§a', 'Ã§eÅŸitli', 'Ã§ok',\n",
    "    'Ã§oklarÄ±', 'Ã§oklarÄ±nca', 'Ã§okluk', 'Ã§oklukla', 'Ã§okÃ§a', 'Ã§oÄŸu', 'Ã§oÄŸun',\n",
    "    'Ã§oÄŸunca', 'Ã§oÄŸunlukla', 'Ã§Ã¼nkÃ¼', 'Ã¶bÃ¼r', 'Ã¶bÃ¼rkÃ¼', 'Ã¶bÃ¼rÃ¼', 'Ã¶nce', 'Ã¶nceden',\n",
    "    'Ã¶nceleri', 'Ã¶ncelikle', 'Ã¶teki', 'Ã¶tekisi', 'Ã¶yle', 'Ã¶ylece', 'Ã¶ylelikle',\n",
    "    'Ã¶ylemesine', 'Ã¶z', 'Ã¼zere', 'Ã¼Ã§', 'ÅŸayet', 'ÅŸey', 'ÅŸeyden', 'ÅŸeyi', 'ÅŸeyler',\n",
    "    'ÅŸu', 'ÅŸuna', 'ÅŸuncacÄ±k', 'ÅŸunda', 'ÅŸundan', 'ÅŸunlar', 'ÅŸunlarÄ±', 'ÅŸunu',\n",
    "    'ÅŸunun', 'ÅŸura', 'ÅŸuracÄ±k', 'ÅŸuracÄ±kta', 'ÅŸurasÄ±', 'ÅŸÃ¶yle', 'ÅŸimdi', 'ÅŸÃ¶yle'\n",
    "}\n",
    "\n",
    "all_stopwords = turkish_stopwords.union(extended_turkish_stopwords)"
   ],
   "id": "2167fb370c3f697b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:56:32.579868Z",
     "start_time": "2025-05-09T11:56:32.536252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Zemberek GRPC baÄŸlantÄ±sÄ±\n",
    "channel = grpc.insecure_channel('localhost:6789')\n",
    "morphology_stub = z_morphology_g.MorphologyServiceStub(channel)\n",
    "\n",
    "# Lemma ve KÃ¶k bulma fonksiyonlarÄ±\n",
    "def get_lemmas(word):\n",
    "    \"\"\"Bir kelimenin lemmalarÄ±nÄ± (kÃ¶klerini) Zemberek ile bulur\"\"\"\n",
    "    try:\n",
    "        response = morphology_stub.AnalyzeWord(z_morphology.WordAnalysisRequest(input=word))\n",
    "        if response.analyses:\n",
    "            return response.analyses[0].lemmas\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_stem(word):\n",
    "    \"\"\"Bir kelimenin kÃ¶kÃ¼nÃ¼ Zemberek ile bulur\"\"\"\n",
    "    lemmas = get_lemmas(word)\n",
    "    return lemmas[0] if lemmas else word\n",
    "# Ã‡oklu kelimeler iÃ§in toplu kÃ¶k bulma (performans iyileÅŸtirmesi)\n",
    "def get_stems_batch(words):\n",
    "    \"\"\"Kelime listesi iÃ§in kÃ¶k bulma iÅŸlemini toplu yapar\"\"\"\n",
    "    results = {}\n",
    "    for word in words:\n",
    "        if word not in results:\n",
    "            results[word] = get_stem(word)\n",
    "    return results\n"
   ],
   "id": "790e872397dcbaf5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:05.131060Z",
     "start_time": "2025-05-09T11:57:04.812049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model yÃ¼kleme\n",
    "model_path = Path(r\"C:\\Users\\Melek\\yapayZeka\\FilmTemaAnaliziProje\\Models\\tam_veri_seti_model4.pkl\")\n",
    "try:\n",
    "    model = joblib.load(model_path)\n",
    "    model_loaded = True\n",
    "except:\n",
    "    print(f\"Model {model_path} konumundan yÃ¼klenemedi. Tahmin yapÄ±lmadan demo olarak Ã§alÄ±ÅŸacaktÄ±r.\")\n",
    "    model_loaded = False\n"
   ],
   "id": "d46bce521b5c3281",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:06.859423Z",
     "start_time": "2025-05-09T11:57:06.853512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Temalar (ilk kodla aynÄ± sÄ±rada)\n",
    "themes = [\n",
    "    \"romantik\", \"savaÅŸ\", \"bilim kurgu\", \"aksiyon\", \"dram\", \"fantastik\", \"gerilim\", \"suÃ§\",\n",
    "    \"tarih\", \"mÃ¼zik\", \"komedi\", \"korku\", \"animasyon\", \"spor\", \"distoptik\", \"polisiye\"\n",
    "]"
   ],
   "id": "e015d1a3b914bb83",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:20.789915Z",
     "start_time": "2025-05-09T11:57:20.780919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_srt_file(file_path):\n",
    "    \"\"\"SRT dosyasÄ±nÄ± temizler (zaman damgalarÄ±nÄ± ve numaralarÄ± kaldÄ±rÄ±r)\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Zaman damgasÄ± satÄ±rlarÄ±nÄ± ve numaralandÄ±rmayÄ± atla\n",
    "        if re.match(r'^\\d+$', line) or '-->' in line:\n",
    "            continue\n",
    "        # BoÅŸ satÄ±rlarÄ± atla\n",
    "        if not line:\n",
    "            if current_line:  # EÄŸer biriktirilen bir satÄ±r varsa ekle\n",
    "                cleaned_lines.append(current_line)\n",
    "                current_line = \"\"\n",
    "            continue\n",
    "\n",
    "        # SatÄ±rlarÄ± birleÅŸtir (altyazÄ±larda cÃ¼mleler genelde birden fazla satÄ±ra bÃ¶lÃ¼nebilir)\n",
    "        if current_line:\n",
    "            current_line += \" \" + line\n",
    "        else:\n",
    "            current_line = line\n",
    "\n",
    "    # Son satÄ±rÄ± eklemeyi unutma\n",
    "    if current_line:\n",
    "        cleaned_lines.append(current_line)\n",
    "\n",
    "    return cleaned_lines"
   ],
   "id": "3e6f9bc79ba1a539",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:31.252396Z",
     "start_time": "2025-05-09T11:57:31.244394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "def split_into_sentences(lines):\n",
    "    \"\"\"Metni cÃ¼mlelere bÃ¶ler\"\"\"\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "        # NLTK'nÄ±n sent_tokenize fonksiyonunu kullanarak daha doÄŸru cÃ¼mle bÃ¶lme\n",
    "        line_sentences = sent_tokenize(line)\n",
    "        for sentence in line_sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    return sentences\n"
   ],
   "id": "71db40dc72f11f4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:41.790365Z",
     "start_time": "2025-05-09T11:57:41.782366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_sentence(sentence):\n",
    "    \"\"\"CÃ¼mleyi temizler\"\"\"\n",
    "    # AltyazÄ± notasyonlarÄ±nÄ± temizle ([gÃ¼lÃ¼ÅŸmeler], (fÄ±sÄ±ldar) gibi)\n",
    "    sentence = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', sentence)\n",
    "\n",
    "    # HTML etiketlerini kaldÄ±r\n",
    "    sentence = re.sub(r'<.*?>', '', sentence)\n",
    "\n",
    "    # Noktalama iÅŸaretlerini kaldÄ±r (sadece kelimeler, sayÄ±lar ve boÅŸluklar kalsÄ±n)\n",
    "    sentence = re.sub(r'[^\\w\\sÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄžÃœÅžÄ°Ã–Ã‡0-9]', ' ', sentence)\n",
    "\n",
    "    # KÃ¼Ã§Ã¼k harfe Ã§evir (sayÄ±lar etkilenmez)\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Fazla boÅŸluklarÄ± kaldÄ±r\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "\n",
    "    return sentence"
   ],
   "id": "e0e9320428fd5d6a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:35:44.655689Z",
     "start_time": "2025-05-08T12:35:44.648685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords_and_stem(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in all_stopwords and (len(word) > 2 or word.isdigit() or re.match(r'^\\w+-\\d+\\.?\\d*$', word)):\n",
    "            if not word.isdigit() and not re.match(r'^\\w+-\\d+\\.?\\d*$', word):\n",
    "                stemmed_word = get_stem(word)\n",
    "                processed_words.append(stemmed_word)\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "    \n",
    "    return ' '.join(processed_words)"
   ],
   "id": "35bba9e7faa085ed",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:58.506448Z",
     "start_time": "2025-05-09T11:57:58.497448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords_and_stem(sentence, stem_cache=None):\n",
    "    \"\"\"Stopwords kaldÄ±rÄ±r ve kÃ¶k bulma iÅŸlemi yapar\"\"\"\n",
    "    if stem_cache is None:\n",
    "        stem_cache = {}\n",
    "\n",
    "    words = word_tokenize(sentence)\n",
    "    processed_words = []\n",
    "\n",
    "    # Cache'de olmayan kelimeler iÃ§in kÃ¶k bulma iÅŸlemi yap\n",
    "    new_words = [w for w in words if w not in stem_cache and w not in all_stopwords\n",
    "                 and (len(w) > 2 or w.isdigit() or re.match(r'^\\w+-\\d+\\.?\\d*$', w))]\n",
    "\n",
    "    if new_words:\n",
    "        new_stems = get_stems_batch(new_words)\n",
    "        stem_cache.update(new_stems)\n",
    "\n",
    "    # CÃ¼mledeki kelimeleri iÅŸle\n",
    "    for word in words:\n",
    "        if word not in all_stopwords and (len(word) > 2 or word.isdigit() or re.match(r'^\\w+-\\d+\\.?\\d*$', word)):\n",
    "            if not word.isdigit() and not re.match(r'^\\w+-\\d+\\.?\\d*$', word):\n",
    "                stemmed_word = stem_cache.get(word, get_stem(word))\n",
    "                processed_words.append(stemmed_word)\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "\n",
    "    return ' '.join(processed_words)"
   ],
   "id": "cb6f67e2c65109ce",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:21.857149Z",
     "start_time": "2025-05-09T12:13:21.839148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_chunks(sentences, chunk_size=25, stride=12):\n",
    "    \"\"\"EÄŸitimde kullanÄ±lan stratejiyle uyumlu overlap'li chunk'lar oluÅŸturur\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences) - chunk_size + 1, stride):\n",
    "        chunk = sentences[i:i + chunk_size]\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    # Son kalan cÃ¼mleleri de ekle (eÄŸitimdeki gibi tamamlanmamÄ±ÅŸ chunk'larÄ± da kullan)\n",
    "    if len(sentences) % chunk_size != 0:\n",
    "        remaining = sentences[-(len(sentences) % chunk_size):]\n",
    "        chunks.append(' '.join(remaining))\n",
    "    return chunks"
   ],
   "id": "585762246defdea7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:44.612180Z",
     "start_time": "2025-05-09T12:13:44.602902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_theme_distribution(chunks):\n",
    "    if not model_loaded:\n",
    "        return {theme: np.random.uniform(0, 1) for theme in themes}\n",
    "\n",
    "    predictions = model.predict(chunks)\n",
    "\n",
    "    theme_counts = {}\n",
    "    for theme in themes:\n",
    "        theme_counts[theme] = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        theme_counts[pred] = theme_counts.get(pred, 0) + 1\n",
    "\n",
    "    total = len(predictions)\n",
    "    theme_percentages = {theme: (count / total) * 100 for theme, count in theme_counts.items()}\n",
    "\n",
    "    return theme_percentages\n"
   ],
   "id": "14b2d456d0b7d5d9",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:52.854698Z",
     "start_time": "2025-05-09T12:13:52.831383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_theme_distribution(theme_percentages):\n",
    "    sorted_themes = sorted(theme_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "    labels = [item[0] for item in sorted_themes]\n",
    "    values = [item[1] for item in sorted_themes]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(labels, values, color='skyblue')\n",
    "    plt.xlabel('Film TemalarÄ±')\n",
    "    plt.ylabel('YÃ¼zdelik (%)')\n",
    "    plt.title('Film Tema DaÄŸÄ±lÄ±mÄ±')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2., height + 0.5,\n",
    "                 f'{height:.1f}%', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    return plt\n"
   ],
   "id": "9ed03e209fc9c973",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:55.707152Z",
     "start_time": "2025-05-09T12:13:55.641304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Frontend.film_analiz_arayuzu import process_subtitle_file\n",
    "\n",
    "\n",
    "def read_file_with_auto_encoding(file_path):\n",
    "    \"\"\"Dosya karakter kodlamasÄ±nÄ± otomatik tespit ederek okur.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "\n",
    "    encodings_to_try = [\n",
    "        encoding,\n",
    "        'utf-8',\n",
    "        'cp1254',\n",
    "        'iso-8859-9',\n",
    "        'latin-5',\n",
    "        'iso-8859-1',\n",
    "        'windows-1252'\n",
    "    ]\n",
    "\n",
    "    for enc in encodings_to_try:\n",
    "        if enc is None:\n",
    "            continue\n",
    "        try:\n",
    "            text = raw_data.decode(enc)\n",
    "            print(f\"{enc} kodlamasÄ±yla baÅŸarÄ±yla Ã§Ã¶zÃ¼ldÃ¼.\")\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"Dosya bilinen hiÃ§bir karakter kodlamasÄ±yla Ã§Ã¶zÃ¼lemedi.\")\n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        if hasattr(file, 'name'):\n",
    "            file_content = read_file_with_auto_encoding(file.name)\n",
    "        else:\n",
    "            if isinstance(file, bytes):\n",
    "                result = chardet.detect(file)\n",
    "                encoding = result['encoding']\n",
    "                try:\n",
    "                    file_content = file.decode(encoding)\n",
    "                except UnicodeDecodeError:\n",
    "                    for enc in ['utf-8', 'cp1254', 'iso-8859-9', 'latin-5', 'iso-8859-1']:\n",
    "                        try:\n",
    "                            file_content = file.decode(enc)\n",
    "                            break\n",
    "                        except UnicodeDecodeError:\n",
    "                            continue\n",
    "                    else:\n",
    "                        raise ValueError(\"Dosya hiÃ§bir yaygÄ±n kodlamayla Ã§Ã¶zÃ¼mlenemedi.\")\n",
    "            elif isinstance(file, str):\n",
    "                file_content = file\n",
    "            else:\n",
    "                raise ValueError(\"Desteklenmeyen dosya formatÄ±. LÃ¼tfen metin tabanlÄ± bir SRT dosyasÄ± yÃ¼kleyin.\")\n",
    "\n",
    "        processed_sentences = process_subtitle_file(file_content)\n",
    "\n",
    "        if not processed_sentences:\n",
    "            return \"Dosyada geÃ§erli bir metin bulunamadÄ±. LÃ¼tfen dosya formatÄ±nÄ± kontrol edin.\", None\n",
    "\n",
    "        chunks = create_chunks(processed_sentences)\n",
    "        theme_percentages = predict_theme_distribution(chunks)\n",
    "        fig = plot_theme_distribution(theme_percentages)\n",
    "\n",
    "        report = \"ðŸŽ¬ Film Tema Analizi SonuÃ§larÄ±:\\n\\n\"\n",
    "        for theme, percentage in sorted(theme_percentages.items(), key=lambda x: x[1], reverse=True):\n",
    "            report += f\"â€¢ {theme.capitalize()}: %{percentage:.1f}\\n\"\n",
    "\n",
    "        return report, fig\n",
    "    except Exception as e:\n",
    "        return f\"Bir hata oluÅŸtu: {str(e)}\", None"
   ],
   "id": "ae30d7403779225c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:14:01.090018Z",
     "start_time": "2025-05-09T12:14:00.567886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradio arayÃ¼zÃ¼ (orijinal kodla aynÄ±)\n",
    "with gr.Blocks(title=\"Film Tema Analizi\") as app:\n",
    "    gr.Markdown(\"# ðŸŽ¬ Film Tema Analizi AracÄ±\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            file_input = gr.File(label=\"AltyazÄ± DosyasÄ± YÃ¼kle (.srt veya .txt)\", file_types=[\".srt\", \".txt\"])\n",
    "            analyze_btn = gr.Button(\"TemalarÄ± Analiz Et\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column():\n",
    "            result_text = gr.Textbox(label=\"Analiz SonuÃ§larÄ±\", lines=12, interactive=False)\n",
    "\n",
    "    chart_output = gr.Plot(label=\"Tema DaÄŸÄ±lÄ±mÄ± GrafiÄŸi\")\n",
    "\n",
    "    analyze_btn.click(\n",
    "        fn=process_file,\n",
    "        inputs=[file_input],\n",
    "        outputs=[result_text, chart_output]\n",
    "    )"
   ],
   "id": "6da034cfc23831e7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:14:08.306041Z",
     "start_time": "2025-05-09T12:14:07.684938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UygulamayÄ± baÅŸlat\n",
    "if __name__ == \"__main__\":\n",
    "    app.launch()"
   ],
   "id": "c668b3863ae3df93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
