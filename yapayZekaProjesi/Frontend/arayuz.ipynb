{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:54:37.292144Z",
     "start_time": "2025-05-09T11:54:20.972777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import chardet\n",
    "import grpc\n",
    "import zemberek_grpc.morphology_pb2 as z_morphology\n",
    "import zemberek_grpc.morphology_pb2_grpc as z_morphology_g\n",
    "from functools import lru_cache"
   ],
   "id": "45e94ff5e922e933",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melek\\PycharmProjects\\MyDataset\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:54:43.606669Z",
     "start_time": "2025-05-09T11:54:37.303666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Türkçe stopwords'ler\n",
    "turkish_stopwords = set(stopwords.words('turkish'))\n",
    "\n",
    "# Genişletilmiş Türkçe stopwords listesi (ilk kodla aynı)\n",
    "extended_turkish_stopwords = {\n",
    "    'a', 'acaba', 'acep', 'adamakıllı', 'adeta', 'ait', 'altmış', 'altı',\n",
    "    'ama', 'amma', 'anca', 'ancak', 'arada', 'artık', 'aslında', 'aynen', 'ayrıca',\n",
    "    'az', 'açıkça', 'açıkçası', 'bana', 'bari', 'bazen', 'bazı', 'başkası',\n",
    "    'belki', 'ben', 'benden', 'beni', 'benim', 'beri', 'beriki', 'beş',\n",
    "    'bilcümle', 'bile', 'bin', 'binaen', 'binaenaleyh', 'bir', 'biraz',\n",
    "    'birazdan', 'birbiri', 'birden', 'birdenbire', 'biri', 'birice', 'birileri',\n",
    "    'birisi', 'birkaç', 'birkaçı', 'birkez', 'birlikte', 'birçok', 'birçoğu',\n",
    "    'bir şey', 'bir şeyi', 'birşey', 'birşeyi', 'bitevi', 'biteviye',\n",
    "    'bittabi', 'biz', 'bizatihi', 'bizce', 'bizcileyin', 'bizden', 'bize', 'bizi',\n",
    "    'bizim', 'bizimki', 'bizzat', 'boşuna', 'bu', 'buna', 'bunda', 'bundan',\n",
    "    'bunlar', 'bunları', 'bunların', 'bunu', 'bunun', 'buracıkta', 'burada',\n",
    "    'buradan', 'burası', 'böyle', 'böylece', 'böylecene', 'böylelikle',\n",
    "    'böylemesine', 'böylesine', 'büsbütün', 'bütün', 'cuk', 'cümlesi', 'da',\n",
    "    'daha', 'dahi', 'dahil', 'dahilen', 'daima', 'dair', 'dayanarak', 'de', 'defa',\n",
    "    'dek', 'demin', 'demincek', 'deminden', 'denli', 'derakap', 'derhal', 'derken',\n",
    "    'değil', 'değin', 'diye', 'diğer', 'diğeri', 'doksan', 'dokuz',\n",
    "    'dolayı', 'dolayısıyla', 'doğru', 'dört', 'edecek', 'eden', 'ederek', 'edilecek',\n",
    "    'ediliyor', 'edilmesi', 'ediyor', 'elbet', 'elbette', 'elli', 'emme', 'en',\n",
    "    'enikonu', 'epey', 'epeyce', 'epeyi', 'esasen', 'esnasında', 'etmesi', 'etraflı',\n",
    "    'etraflíca', 'etti', 'ettiği', 'ettiğini', 'evleviyetle', 'evvel', 'evvela',\n",
    "    'evvelce', 'evvelden', 'evvelemirde', 'evveli', 'eđer', 'eğer', 'fakat',\n",
    "    'filanca', 'gah', 'gayet', 'gayetle', 'gayri', 'gayrı', 'gelgelelim', 'gene',\n",
    "    'gerek', 'gerçi', 'geçende', 'geçenlerde', 'gibi', 'gibilerden', 'gibisinden',\n",
    "    'gine', 'göre', 'gırla', 'hakeza', 'halbuki', 'halen', 'halihazırda', 'haliyle',\n",
    "    'handiyse', 'hangi', 'hangisi', 'hani', 'hariç', 'hasebiyle', 'hasılı', 'hatta',\n",
    "    'hele', 'hem', 'henüz', 'hep', 'hepsi', 'her', 'herhangi', 'herkes', 'herkesin',\n",
    "    'hiç', 'hiçbir', 'hiçbiri', 'hoş', 'hulasaten', 'iken', 'iki', 'ila', 'ile',\n",
    "    'ilen', 'ilgili', 'ilk', 'illa', 'illaki', 'imdi', 'indinde', 'inen', 'insermi',\n",
    "    'ise', 'ister', 'itibaren', 'itibariyle', 'itibarıyla', 'iyi', 'iyice', 'iyicene',\n",
    "    'için', 'iş', 'işte', 'kadar', 'kaffesi', 'kah', 'kala', 'kannımca',\n",
    "    'karşın', 'katrilyon', 'kaynak', 'kaçı', 'kelli', 'kendi', 'kendilerine',\n",
    "    'kendini', 'kendisi', 'kendisine', 'kendisini', 'kere', 'kez', 'keza',\n",
    "    'kezalik', 'keşke', 'keţke', 'ki', 'kim', 'kimden', 'kime', 'kimi', 'kimisi',\n",
    "    'kimse', 'kimsecik', 'kimsecikler', 'külliyen', 'kırk',\n",
    "    'kısaca', 'lakin', 'leh', 'lütfen', 'maada', 'madem', 'mademki', 'mamafih',\n",
    "    'mebni', 'meğer', 'meğerki', 'meğerse', 'milyar', 'milyon', 'mu',\n",
    "    'mü', 'mi', 'mı', 'nasıl', 'nasılsa', 'nazaran', 'naşi', 'ne', 'neden',\n",
    "    'nedeniyle', 'nedenle', 'nedense', 'nerde', 'nerden', 'nerdeyse', 'nere',\n",
    "    'nerede', 'nereden', 'neredeyse', 'neresi', 'nereye', 'netekim', 'neye', 'neyi',\n",
    "    'neyse', 'nice', 'nihayet', 'nihayetinde', 'nitekim', 'niye', 'niçin', 'o',\n",
    "    'olan', 'olarak', 'oldu', 'olduklarını', 'oldukça', 'olduğu', 'olduğunu',\n",
    "    'olmadı', 'olmadığı', 'olmak', 'olması', 'olmayan', 'olmaz', 'olsa', 'olsun',\n",
    "    'olup', 'olur', 'olursa', 'oluyor', 'on', 'ona', 'onca', 'onculayın', 'onda',\n",
    "    'ondan', 'onlar', 'onlardan', 'onları', 'onların', 'onu',\n",
    "    'onun', 'oracık', 'oracıkta', 'orada', 'oradan', 'oranca', 'oranla', 'oraya',\n",
    "    'otuz', 'oysa', 'oysaki', 'pek', 'pekala', 'peki', 'pekçe', 'peyderpey', 'rağmen',\n",
    "    'sadece', 'sahi', 'sahiden', 'sana', 'sanki', 'sekiz', 'seksen', 'sen', 'senden',\n",
    "    'seni', 'senin', 'siz', 'sizden', 'sizi', 'sizin', 'sonra', 'sonradan',\n",
    "    'sonraları', 'sonunda', 'tabii', 'tam', 'tamam', 'tamamen', 'tamamıyla',\n",
    "    'tarafından', 'tek', 'trilyon', 'tüm', 'var', 'vardı', 'vasıtasıyla', 've',\n",
    "    'velev', 'velhasıl', 'velhasılıkelam', 'veya', 'veyahut', 'ya', 'yahut',\n",
    "    'yakinen', 'yakında', 'yakından', 'yakınlarda', 'yalnız', 'yalnızca', 'yani',\n",
    "    'yapacak', 'yapmak', 'yaptı', 'yaptıkları', 'yaptığı', 'yaptığını', 'yapılan',\n",
    "    'yapılması', 'yapıyor', 'yedi', 'yeniden', 'yenilerde', 'yerine',\n",
    "    'yetmiş', 'yine', 'yirmi', 'yok', 'yoksa', 'yoluyla', 'yüz', 'yüzünden',\n",
    "    'zarfında', 'zaten', 'zati', 'zira', 'çabuk', 'çabukça', 'çeşitli', 'çok',\n",
    "    'çokları', 'çoklarınca', 'çokluk', 'çoklukla', 'çokça', 'çoğu', 'çoğun',\n",
    "    'çoğunca', 'çoğunlukla', 'çünkü', 'öbür', 'öbürkü', 'öbürü', 'önce', 'önceden',\n",
    "    'önceleri', 'öncelikle', 'öteki', 'ötekisi', 'öyle', 'öylece', 'öylelikle',\n",
    "    'öylemesine', 'öz', 'üzere', 'üç', 'şayet', 'şey', 'şeyden', 'şeyi', 'şeyler',\n",
    "    'şu', 'şuna', 'şuncacık', 'şunda', 'şundan', 'şunlar', 'şunları', 'şunu',\n",
    "    'şunun', 'şura', 'şuracık', 'şuracıkta', 'şurası', 'şöyle', 'şimdi', 'şöyle'\n",
    "}\n",
    "\n",
    "all_stopwords = turkish_stopwords.union(extended_turkish_stopwords)"
   ],
   "id": "2167fb370c3f697b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:56:32.579868Z",
     "start_time": "2025-05-09T11:56:32.536252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Zemberek GRPC bağlantısı\n",
    "channel = grpc.insecure_channel('localhost:6789')\n",
    "morphology_stub = z_morphology_g.MorphologyServiceStub(channel)\n",
    "\n",
    "# Lemma ve Kök bulma fonksiyonları\n",
    "def get_lemmas(word):\n",
    "    \"\"\"Bir kelimenin lemmalarını (köklerini) Zemberek ile bulur\"\"\"\n",
    "    try:\n",
    "        response = morphology_stub.AnalyzeWord(z_morphology.WordAnalysisRequest(input=word))\n",
    "        if response.analyses:\n",
    "            return response.analyses[0].lemmas\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_stem(word):\n",
    "    \"\"\"Bir kelimenin kökünü Zemberek ile bulur\"\"\"\n",
    "    lemmas = get_lemmas(word)\n",
    "    return lemmas[0] if lemmas else word\n",
    "# Çoklu kelimeler için toplu kök bulma (performans iyileştirmesi)\n",
    "def get_stems_batch(words):\n",
    "    \"\"\"Kelime listesi için kök bulma işlemini toplu yapar\"\"\"\n",
    "    results = {}\n",
    "    for word in words:\n",
    "        if word not in results:\n",
    "            results[word] = get_stem(word)\n",
    "    return results\n"
   ],
   "id": "790e872397dcbaf5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:05.131060Z",
     "start_time": "2025-05-09T11:57:04.812049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model yükleme\n",
    "model_path = Path(r\"C:\\Users\\Melek\\yapayZeka\\FilmTemaAnaliziProje\\Models\\tam_veri_seti_model4.pkl\")\n",
    "try:\n",
    "    model = joblib.load(model_path)\n",
    "    model_loaded = True\n",
    "except:\n",
    "    print(f\"Model {model_path} konumundan yüklenemedi. Tahmin yapılmadan demo olarak çalışacaktır.\")\n",
    "    model_loaded = False\n"
   ],
   "id": "d46bce521b5c3281",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:06.859423Z",
     "start_time": "2025-05-09T11:57:06.853512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Temalar (ilk kodla aynı sırada)\n",
    "themes = [\n",
    "    \"romantik\", \"savaş\", \"bilim kurgu\", \"aksiyon\", \"dram\", \"fantastik\", \"gerilim\", \"suç\",\n",
    "    \"tarih\", \"müzik\", \"komedi\", \"korku\", \"animasyon\", \"spor\", \"distoptik\", \"polisiye\"\n",
    "]"
   ],
   "id": "e015d1a3b914bb83",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:20.789915Z",
     "start_time": "2025-05-09T11:57:20.780919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_srt_file(file_path):\n",
    "    \"\"\"SRT dosyasını temizler (zaman damgalarını ve numaraları kaldırır)\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Zaman damgası satırlarını ve numaralandırmayı atla\n",
    "        if re.match(r'^\\d+$', line) or '-->' in line:\n",
    "            continue\n",
    "        # Boş satırları atla\n",
    "        if not line:\n",
    "            if current_line:  # Eğer biriktirilen bir satır varsa ekle\n",
    "                cleaned_lines.append(current_line)\n",
    "                current_line = \"\"\n",
    "            continue\n",
    "\n",
    "        # Satırları birleştir (altyazılarda cümleler genelde birden fazla satıra bölünebilir)\n",
    "        if current_line:\n",
    "            current_line += \" \" + line\n",
    "        else:\n",
    "            current_line = line\n",
    "\n",
    "    # Son satırı eklemeyi unutma\n",
    "    if current_line:\n",
    "        cleaned_lines.append(current_line)\n",
    "\n",
    "    return cleaned_lines"
   ],
   "id": "3e6f9bc79ba1a539",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:31.252396Z",
     "start_time": "2025-05-09T11:57:31.244394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "def split_into_sentences(lines):\n",
    "    \"\"\"Metni cümlelere böler\"\"\"\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "        # NLTK'nın sent_tokenize fonksiyonunu kullanarak daha doğru cümle bölme\n",
    "        line_sentences = sent_tokenize(line)\n",
    "        for sentence in line_sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    return sentences\n"
   ],
   "id": "71db40dc72f11f4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:41.790365Z",
     "start_time": "2025-05-09T11:57:41.782366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_sentence(sentence):\n",
    "    \"\"\"Cümleyi temizler\"\"\"\n",
    "    # Altyazı notasyonlarını temizle ([gülüşmeler], (fısıldar) gibi)\n",
    "    sentence = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', sentence)\n",
    "\n",
    "    # HTML etiketlerini kaldır\n",
    "    sentence = re.sub(r'<.*?>', '', sentence)\n",
    "\n",
    "    # Noktalama işaretlerini kaldır (sadece kelimeler, sayılar ve boşluklar kalsın)\n",
    "    sentence = re.sub(r'[^\\w\\sğüşıöçĞÜŞİÖÇ0-9]', ' ', sentence)\n",
    "\n",
    "    # Küçük harfe çevir (sayılar etkilenmez)\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Fazla boşlukları kaldır\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "\n",
    "    return sentence"
   ],
   "id": "e0e9320428fd5d6a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:35:44.655689Z",
     "start_time": "2025-05-08T12:35:44.648685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords_and_stem(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in all_stopwords and (len(word) > 2 or word.isdigit() or re.match(r'^\\w+-\\d+\\.?\\d*$', word)):\n",
    "            if not word.isdigit() and not re.match(r'^\\w+-\\d+\\.?\\d*$', word):\n",
    "                stemmed_word = get_stem(word)\n",
    "                processed_words.append(stemmed_word)\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "    \n",
    "    return ' '.join(processed_words)"
   ],
   "id": "35bba9e7faa085ed",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:57:58.506448Z",
     "start_time": "2025-05-09T11:57:58.497448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords_and_stem(sentence, stem_cache=None):\n",
    "    \"\"\"Stopwords kaldırır ve kök bulma işlemi yapar\"\"\"\n",
    "    if stem_cache is None:\n",
    "        stem_cache = {}\n",
    "\n",
    "    words = word_tokenize(sentence)\n",
    "    processed_words = []\n",
    "\n",
    "    # Cache'de olmayan kelimeler için kök bulma işlemi yap\n",
    "    new_words = [w for w in words if w not in stem_cache and w not in all_stopwords\n",
    "                 and (len(w) > 2 or w.isdigit() or re.match(r'^\\w+-\\d+\\.?\\d*$', w))]\n",
    "\n",
    "    if new_words:\n",
    "        new_stems = get_stems_batch(new_words)\n",
    "        stem_cache.update(new_stems)\n",
    "\n",
    "    # Cümledeki kelimeleri işle\n",
    "    for word in words:\n",
    "        if word not in all_stopwords and (len(word) > 2 or word.isdigit() or re.match(r'^\\w+-\\d+\\.?\\d*$', word)):\n",
    "            if not word.isdigit() and not re.match(r'^\\w+-\\d+\\.?\\d*$', word):\n",
    "                stemmed_word = stem_cache.get(word, get_stem(word))\n",
    "                processed_words.append(stemmed_word)\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "\n",
    "    return ' '.join(processed_words)"
   ],
   "id": "cb6f67e2c65109ce",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:21.857149Z",
     "start_time": "2025-05-09T12:13:21.839148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_chunks(sentences, chunk_size=25, stride=12):\n",
    "    \"\"\"Eğitimde kullanılan stratejiyle uyumlu overlap'li chunk'lar oluşturur\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences) - chunk_size + 1, stride):\n",
    "        chunk = sentences[i:i + chunk_size]\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    # Son kalan cümleleri de ekle (eğitimdeki gibi tamamlanmamış chunk'ları da kullan)\n",
    "    if len(sentences) % chunk_size != 0:\n",
    "        remaining = sentences[-(len(sentences) % chunk_size):]\n",
    "        chunks.append(' '.join(remaining))\n",
    "    return chunks"
   ],
   "id": "585762246defdea7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:44.612180Z",
     "start_time": "2025-05-09T12:13:44.602902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_theme_distribution(chunks):\n",
    "    if not model_loaded:\n",
    "        return {theme: np.random.uniform(0, 1) for theme in themes}\n",
    "\n",
    "    predictions = model.predict(chunks)\n",
    "\n",
    "    theme_counts = {}\n",
    "    for theme in themes:\n",
    "        theme_counts[theme] = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        theme_counts[pred] = theme_counts.get(pred, 0) + 1\n",
    "\n",
    "    total = len(predictions)\n",
    "    theme_percentages = {theme: (count / total) * 100 for theme, count in theme_counts.items()}\n",
    "\n",
    "    return theme_percentages\n"
   ],
   "id": "14b2d456d0b7d5d9",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:52.854698Z",
     "start_time": "2025-05-09T12:13:52.831383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_theme_distribution(theme_percentages):\n",
    "    sorted_themes = sorted(theme_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "    labels = [item[0] for item in sorted_themes]\n",
    "    values = [item[1] for item in sorted_themes]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(labels, values, color='skyblue')\n",
    "    plt.xlabel('Film Temaları')\n",
    "    plt.ylabel('Yüzdelik (%)')\n",
    "    plt.title('Film Tema Dağılımı')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2., height + 0.5,\n",
    "                 f'{height:.1f}%', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    return plt\n"
   ],
   "id": "9ed03e209fc9c973",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:13:55.707152Z",
     "start_time": "2025-05-09T12:13:55.641304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Frontend.film_analiz_arayuzu import process_subtitle_file\n",
    "\n",
    "\n",
    "def read_file_with_auto_encoding(file_path):\n",
    "    \"\"\"Dosya karakter kodlamasını otomatik tespit ederek okur.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "\n",
    "    encodings_to_try = [\n",
    "        encoding,\n",
    "        'utf-8',\n",
    "        'cp1254',\n",
    "        'iso-8859-9',\n",
    "        'latin-5',\n",
    "        'iso-8859-1',\n",
    "        'windows-1252'\n",
    "    ]\n",
    "\n",
    "    for enc in encodings_to_try:\n",
    "        if enc is None:\n",
    "            continue\n",
    "        try:\n",
    "            text = raw_data.decode(enc)\n",
    "            print(f\"{enc} kodlamasıyla başarıyla çözüldü.\")\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"Dosya bilinen hiçbir karakter kodlamasıyla çözülemedi.\")\n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        if hasattr(file, 'name'):\n",
    "            file_content = read_file_with_auto_encoding(file.name)\n",
    "        else:\n",
    "            if isinstance(file, bytes):\n",
    "                result = chardet.detect(file)\n",
    "                encoding = result['encoding']\n",
    "                try:\n",
    "                    file_content = file.decode(encoding)\n",
    "                except UnicodeDecodeError:\n",
    "                    for enc in ['utf-8', 'cp1254', 'iso-8859-9', 'latin-5', 'iso-8859-1']:\n",
    "                        try:\n",
    "                            file_content = file.decode(enc)\n",
    "                            break\n",
    "                        except UnicodeDecodeError:\n",
    "                            continue\n",
    "                    else:\n",
    "                        raise ValueError(\"Dosya hiçbir yaygın kodlamayla çözümlenemedi.\")\n",
    "            elif isinstance(file, str):\n",
    "                file_content = file\n",
    "            else:\n",
    "                raise ValueError(\"Desteklenmeyen dosya formatı. Lütfen metin tabanlı bir SRT dosyası yükleyin.\")\n",
    "\n",
    "        processed_sentences = process_subtitle_file(file_content)\n",
    "\n",
    "        if not processed_sentences:\n",
    "            return \"Dosyada geçerli bir metin bulunamadı. Lütfen dosya formatını kontrol edin.\", None\n",
    "\n",
    "        chunks = create_chunks(processed_sentences)\n",
    "        theme_percentages = predict_theme_distribution(chunks)\n",
    "        fig = plot_theme_distribution(theme_percentages)\n",
    "\n",
    "        report = \"🎬 Film Tema Analizi Sonuçları:\\n\\n\"\n",
    "        for theme, percentage in sorted(theme_percentages.items(), key=lambda x: x[1], reverse=True):\n",
    "            report += f\"• {theme.capitalize()}: %{percentage:.1f}\\n\"\n",
    "\n",
    "        return report, fig\n",
    "    except Exception as e:\n",
    "        return f\"Bir hata oluştu: {str(e)}\", None"
   ],
   "id": "ae30d7403779225c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:14:01.090018Z",
     "start_time": "2025-05-09T12:14:00.567886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradio arayüzü (orijinal kodla aynı)\n",
    "with gr.Blocks(title=\"Film Tema Analizi\") as app:\n",
    "    gr.Markdown(\"# 🎬 Film Tema Analizi Aracı\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            file_input = gr.File(label=\"Altyazı Dosyası Yükle (.srt veya .txt)\", file_types=[\".srt\", \".txt\"])\n",
    "            analyze_btn = gr.Button(\"Temaları Analiz Et\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column():\n",
    "            result_text = gr.Textbox(label=\"Analiz Sonuçları\", lines=12, interactive=False)\n",
    "\n",
    "    chart_output = gr.Plot(label=\"Tema Dağılımı Grafiği\")\n",
    "\n",
    "    analyze_btn.click(\n",
    "        fn=process_file,\n",
    "        inputs=[file_input],\n",
    "        outputs=[result_text, chart_output]\n",
    "    )"
   ],
   "id": "6da034cfc23831e7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:14:08.306041Z",
     "start_time": "2025-05-09T12:14:07.684938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uygulamayı başlat\n",
    "if __name__ == \"__main__\":\n",
    "    app.launch()"
   ],
   "id": "c668b3863ae3df93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
