{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T11:44:28.061344Z",
     "start_time": "2025-05-09T11:44:26.511707Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "import grpc\n",
    "\n",
    "import zemberek_grpc.language_id_pb2 as z_langid\n",
    "import zemberek_grpc.language_id_pb2_grpc as z_langid_g\n",
    "import zemberek_grpc.normalization_pb2 as z_normalization\n",
    "import zemberek_grpc.normalization_pb2_grpc as z_normalization_g\n",
    "import zemberek_grpc.preprocess_pb2 as z_preprocess\n",
    "import zemberek_grpc.preprocess_pb2_grpc as z_preprocess_g\n",
    "import zemberek_grpc.morphology_pb2 as z_morphology\n",
    "import zemberek_grpc.morphology_pb2_grpc as z_morphology_g\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:6789')\n",
    "\n",
    "langid_stub = z_langid_g.LanguageIdServiceStub(channel)\n",
    "normalization_stub = z_normalization_g.NormalizationServiceStub(channel)\n",
    "preprocess_stub = z_preprocess_g.PreprocessingServiceStub(channel)\n",
    "morphology_stub = z_morphology_g.MorphologyServiceStub(channel)\n",
    "\n",
    "def find_lang_id(i):\n",
    "    response = langid_stub.Detect(z_langid.LanguageIdRequest(input=i))\n",
    "    return response.langId\n",
    "\n",
    "def tokenize(i):\n",
    "    response = preprocess_stub.Tokenize(z_preprocess.TokenizationRequest(input=i))\n",
    "    return response.tokens\n",
    "\n",
    "def normalize(i):\n",
    "    response = normalization_stub.Normalize(z_normalization.NormalizationRequest(input=i))\n",
    "    return response\n",
    "\n",
    "def analyze(i):\n",
    "    response = morphology_stub.AnalyzeSentence(z_morphology.SentenceAnalysisRequest(input=i))\n",
    "    return response;\n",
    "\n",
    "def fix_decode(text):\n",
    "    \"\"\"Pass decode.\"\"\"\n",
    "    if sys.version_info < (3, 0):\n",
    "        return text.decode('utf-8')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def run():\n",
    "    lang_detect_input = 'merhaba dünya'\n",
    "    lang_id = find_lang_id(lang_detect_input)\n",
    "    print(\"Language of [\" + fix_decode(lang_detect_input) + \"] is: \" + lang_id)\n",
    "\n",
    "    print(\"\")\n",
    "    tokenization_input = 'Merhaba dünya!'\n",
    "    print('Tokens for input : ' + fix_decode(tokenization_input))\n",
    "    tokens = tokenize(tokenization_input)\n",
    "    for t in tokens:\n",
    "        print(t.token + ':' + t.type)\n",
    "\n",
    "    print(\"\")\n",
    "    normalization_input = 'Mrhaba dnya'\n",
    "    print('Normalization result for input : ' + fix_decode(normalization_input))\n",
    "    n_response = normalize(normalization_input)\n",
    "    if n_response.normalized_input:\n",
    "        print(n_response.normalized_input)\n",
    "    else:\n",
    "        print('Problem normalizing input : ' + n_response.error)\n",
    "\n",
    "    print(\"\")\n",
    "    analysis_input = 'Kavanozun kapağını açamadım.'\n",
    "    print('Analysis result for input : ' + fix_decode(analysis_input))\n",
    "    analysis_result = analyze(analysis_input)\n",
    "    for a in analysis_result.results:\n",
    "        best = a.best\n",
    "        lemmas = \"\"\n",
    "        for l in best.lemmas:\n",
    "          lemmas = lemmas + \" \" + l\n",
    "        print(\"Word = \" + a.token + \", Lemmas = \" + lemmas + \", POS = [\" + best.pos + \"], Full Analysis = {\" + best.analysis + \"}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language of [merhaba dünya] is: tr\n",
      "\n",
      "Tokens for input : Merhaba dünya!\n",
      "Merhaba:Word\n",
      "dünya:Word\n",
      "!:Punctuation\n",
      "\n",
      "Normalization result for input : Mrhaba dnya\n",
      "merhaba dünya\n",
      "\n",
      "Analysis result for input : Kavanozun kapağını açamadım.\n",
      "Word = Kavanozun, Lemmas =  kavanoz, POS = [Noun], Full Analysis = {[kavanoz:Noun] kavanoz:Noun+A3sg+un:Gen}\n",
      "Word = kapağını, Lemmas =  kapak, POS = [Noun], Full Analysis = {[kapak:Noun] kapağ:Noun+A3sg+ı:P3sg+nı:Acc}\n",
      "Word = açamadım, Lemmas =  aç, POS = [Verb], Full Analysis = {[açmak:Verb] aç:Verb+ama:Unable+dı:Past+m:A1sg}\n",
      "Word = ., Lemmas =  ., POS = [Punc], Full Analysis = {[.:Punc] .:Punc}\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
